{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Extracting the loans data from the cloud\n",
    "\n",
    "A db_utils.py file is created to contain the code that connects with the AWS RDS cloud engine using the database credentials, extracts the loans data from the cloud and then finally saves the data locally in .csv format. Credentials.yaml file (containing RDS database credentials) is stored locally and ignored by GitHub for security reasons.\n",
    "<br>\n",
    "<br>\n",
    "<ins>*The Python code consists of three parts:*</ins>\n",
    " - **load_credentials_file() function.** The function when called returns the credentials information presented in a dictionary format.\n",
    " - **class RDSDatabaseConnector.** The class contains *create_engine()* method that creates an engine to connect to the remote dabatase and extract_RDS_data(engine) method that uses that engine to extract the loans data, formats it into Pandas DataFrame and returns it as 'data' variable (where the loans data is contained).\n",
    " - **save_to_csv() function.** The function when called the data extracted from the remote RDS database is saved locally in .csv format and named \"loan_payments\".</ul>\n",
    "<br>\n",
    "RDSDatabaseConnector is using the AWS RDS database credentials information, which is locally stored in credentials.yaml file (and ignored by GitHub), to connect with the cloud server and then extract the loans data information for temporary viewing. The *save_to_csv()* function, when called, saves all the loans data locally at the chosen location in order to have easier access to the read, modify and analyse and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_credentials_file():\n",
    "    with open('credentials.yaml', 'r') as f:\n",
    "        cred = yaml.safe_load(f)\n",
    "    return cred\n",
    "\n",
    "credentials = load_credentials_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_utils import RDSDatabaseConnector\n",
    "\n",
    "db_connect = RDSDatabaseConnector(credentials)\n",
    "\n",
    "engine = db_connect.create_engine()\n",
    "\n",
    "df = db_connect.extract_RDS_data(engine)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv():\n",
    "    df.to_csv(\"C:/Users/eveli/ai_core/EDA/loan_payments.csv\")\n",
    "    \n",
    "save_to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the locally saved loans data.\n",
    "<br>\n",
    "The loans data is loaded and stored in a variable called df_raw (short for \"raw dataframe\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw = pd.read_csv(\"C:/Users/eveli/ai_core/EDA/loan_payments.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Importing classes necessary for the work with the loans dataframe. \n",
    "\n",
    "Four classes are imported from separate python files:\n",
    " - Class DataTransform: changes the data types of columns that have been assigned with the wrong data type as well as rounds the values in columns to two decimal places.\n",
    " - Class DataFrameInfo: provides useful information about the data frame for the purpose of familiarising with the data before the cleaning process. \n",
    " - Class Plotter: provides visual insights of the data.\n",
    " - Class DataFrameTransform: contains various methods used for cleaning and transformation of the data frame (such as methods to deal with NaN values, to correct skewness of the data, removal or transformation of outliers and removal of correlated columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from class_data_transform import DataTransform\n",
    "\n",
    "from class_data_frame_info import DataFrameInfo\n",
    "\n",
    "from class_plotter import Plotter\n",
    "\n",
    "from class_data_frame_transform import DataFrameTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract insightful information from the data.\n",
    "\n",
    "Class DataFrameInfo is created to generate useful information about the data which helps us to familiarise ourselves with it. \n",
    "<br>\n",
    "There are seven methods in the class:\n",
    " - show_colm_dtypes()\n",
    " - show_colm_head()\n",
    " - show_colm_tail()\n",
    " - show_colm_stat_values()\n",
    " - show_categ_colm_values_count()\n",
    " - show_df_shape()\n",
    " - show_null_percentage()</ul>\n",
    "<br>\n",
    "<ins>The first method</ins> (*show_colm_dtypes()*) shown below lists the name of every column in the data frame (including the index column *\"Unnamed\"* created by pandas), the count of non-null values each column has and, most importantly, what data type each column is assigned. This information helps us to establish whether any of the columns were assigned with the wrong data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = DataFrameInfo(df_raw)\n",
    "\n",
    "info.show_colm_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The second method</ins> (*show_colm_head()*) shows the values of the top 25 rows of each column (categorical data columns are excluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_colm_head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The third method</ins> (*show_colm_tail()*) shows the values of the bottom 25 rows of each column (categorical data columns are excluded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_colm_tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The fourth method</ins> (*show_colm_stat_values()*) generates statistical values such as mean, standard deviation, count of data entries, min & max values as well as 25/50/75 percentiles for each column (includes only the columns containing continuous values).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_colm_stat_values() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The fifth method</ins> (*show_categ_colm_values_count()*) shows the count of data entries for each value in categorical columns (the columns containing categorical data). \n",
    "<br>\n",
    "This helps to group the data entries of each column into groups for better insight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_categ_colm_values_count() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The sixth method</ins> (*show_df_shape()*) shows the shape of the data frame. In this case, the shape of our data frame is (54231, 44) meaning it contains 44 columns and 54231 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_df_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>The last (seventh) method</ins> (*show_null_percentage()*) shows the percentage of null values in each column. This information will be essential in finding out which columns contains null values and based on the percentage of null values, in choosing which cleaning method is more appropriate in dealing with the null values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_null_percentage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Convert columns to the correct data format.\n",
    "After using class DataFrameInfo to analyse the given raw data, it was noticed that some of the columns could be assigned with better data format. For this purpose class DataTransform was created. \n",
    "<br>\n",
    "<br>\n",
    "There are three methods in the class:\n",
    "\n",
    " - float_to_int()\n",
    " - object_to_datetime()\n",
    " - round_values()</ul>\n",
    "<br>\n",
    "<ins>The first method</ins> (*float_to_int()*) transforms \"funded_amount_inv\" and \"annual_inc\" columns from float ('float64') to integer ('int64') data type as the data in these columns only consists of integer values. There are other columns such as \"funded_amount\", \"mths_since_last_delinq\", mths_since_last_record\", \"collections_12_mths_ex_med\" and \"mths_since_last_major_derog\" that could benefit with this exact transformation as well however since they contain Nan values, the transformation would be returned with ValueError error. This is because NaN values are not considered valid integers, and therefore cannot be converted to integers.\n",
    "<br>\n",
    "<br>\n",
    "<ins>The second method</ins> (*object_to_datetime()*) transforms \"issue_date\", \"earliest_credit_line\", \"last_payment_date\", \"next_payment_date\" and \"last_credit_pull_date\" columns from string ('object') to datetime ('datetime64') data type as the data in these columns represents dates. \n",
    "<br>\n",
    "<br>\n",
    "<ins>The third method</ins> (*round_values()*) rounds the values in \"total_rec_late_fee\" and \"collection_recovery_fee\" columns by two decimal values as the values represents a monetary value which by convention is expressed only up to two decimal values (for example, 22.23 value meaning 22 pounds and 23 pence).\n",
    "<br>\n",
    "<br>\n",
    "After all the methods are called, we use *show_colm_dtypes()* method from class DataFrameInfo to check whether the data types of the columns were successfully changed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = DataTransform(df_raw)\n",
    "\n",
    "columns_list = [\"funded_amount_inv\", \"annual_inc\"]\n",
    "for column in columns_list:\n",
    "    transform.float_to_int(column)\n",
    "\n",
    "col_list = [\"issue_date\", \"earliest_credit_line\", \"last_payment_date\", \"next_payment_date\", \"last_credit_pull_date\"]\n",
    "for column in col_list:\n",
    "    transform.object_to_datetime(column)\n",
    "\n",
    "colmn_list = [\"total_rec_late_fee\", \"collection_recovery_fee\"]\n",
    "for column in colmn_list:\n",
    "    transform.round_values(column)\n",
    "\n",
    "info.show_colm_dtypes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Remove or impute missing values in the data.\n",
    "\n",
    "Missing values in the dataset are a common phenomenon (especially in a large dataset like the current one) and it can be caused by errors in data collections, respondents not providing information or/and observations not being recorded. It is essential to deal with missing values (expressed in the dataset as 'Nan') as leaving them would make it hard to analyse the data (due to reasons such as creating biased results or many machine algoriths not being able to parse null values).\n",
    "\n",
    "We will be using the class Plotter to visualise the Nan values and in this way identify in which columns they are present as well as class DataFrameTransform which will be dealing with the missing values by either imputing or removing them. \n",
    "\n",
    "By calling *null_values* (a method from class Plotter) we can see the percentage of Nan values that consists in each column and then a graphical representation of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals = Plotter(df_raw)\n",
    "\n",
    "null_values = visuals.show_null_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows us that 11 columns contain Nan values:\n",
    "\n",
    " - \"funded_amount\" (5.5%)\n",
    " - \"term\" (8.8%)\n",
    " - \"int_rate\" (9.5%)\n",
    " - \"employment_length\" (3.9%)\n",
    " - \"mths_since_last_delinq\" (57.2%)\n",
    " - \"mths_since_last_record\" (88.6%)\n",
    " - \"last_payment_date\" (0.13%)\n",
    " - \"next_payment_date\" (60.1%)\n",
    " - \"last_credit_pull_date\" (0.01%)\n",
    " - \"collections_12_mths_ex_med\" (0.09%)\n",
    " - \"mths_since_last_major_derog\" (86.2%)</ul>\n",
    "<br> \n",
    "Columns containing continuous data such as \"funded_amount\" and \"int_rate\" have just under 10% of Nan values. This number is too large to apply dropping rows (that contains Nan values) method and too small to just drop the entire columns, which means the Nan values will need to be imputed instead. The Nan values can be imputed with mean if the data is normally distributed or with median if the data is skewed. We will call the class Plotter *test_normal_distribution()* method to test the normal distribution and see the graphical illustration of the distribution for these two columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd_columns = [\"funded_amount\", \"int_rate\"]\n",
    "for colm in nd_columns:\n",
    "    visuals.test_normal_distribution(colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graphs above we can see the data in \"funded_amount\" and \"int_rate\" columns is skewed therefore we will impute the Nan values with the median rather than the mean. This can be done by calling *impute_with_median()* method from class DataFrameTransform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform = DataFrameTransform(df_raw)\n",
    "\n",
    "median_columns = [\"funded_amount\", \"int_rate\"]\n",
    "for column in median_columns:\n",
    "    df_transform.impute_with_median(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output produced by the method above states that both \"funded_amount\" and \"int_rate\" columns contain 54231 of non-null values which is the total number of rows contained in each column. This proves that Nan values in these columns have been dealt with. \n",
    "<br>\n",
    "<br>\n",
    "Since \"term\" and \"employment_length\" are categorical columns and also have relatively high percentage of Nan values (8.8% and 3.9% respectively), we should not drop the rows with Nan values (as this will make us lose quite significant amount of data) but instead impute them with mode (mean or median should only be used for continuous data). This can be done by calling *impute_with_mode()* method from class DataFrameTransform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_columns = [\"term\", \"employment_length\"]\n",
    "for mod_column in mode_columns:\n",
    "    df_transform.impute_with_mode(mod_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again the output produced by the method above states that \"term\" and \"employment_length\" columns contain 54231 of non-null values. This proves that Nan values in these columns have been dealt with. \n",
    "<br>\n",
    "\n",
    "The \"last_payment_date\", \"last_credit_pull_date\" and \"collections_12_mths_ex_med\" columns contain less or equal to 0.1% of Nan values. As the percentage is so small, it is safe to drop the rows containing Nan values for these columns. This can be done by calling *drop_rows()* method from class DataFrameTransform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_drop_columns = [\"last_payment_date\", \"last_credit_pull_date\", \"collections_12_mths_ex_med\"]\n",
    "for row_column in row_drop_columns:\n",
    "    df_transform.drop_rows(row_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the above method, the rows containing Nan values had to be dropped, we are observing the decrease in total row number/non-null values. We can see that 73 rows containing Nan values were dropped in \"last_payment_date\" (54231-54158=73), 7 rows were dropped in \"last_credit_pull_date\" (54158-54151=7) and 51 rows were dropped in \"collections_12_mths_ex_med\" (54151-54100=51). This should leave us with a total of 54100 rows in the dataset. We can check this by calling the *show_df_shape()* method from class DataFrameInfo which shows the current shape of the data frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_df_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the remaining \"mths_since_last_delinq\", \"mths_since_last_record\", \"next_payment_date\" and \"mths_since_last_major_derog\" columns contain more than 50% of Nan values which means we cannot impute the Nan values (as this would mean filling more than half of the data with imputed values) or drop the rows containing Nan values (as this would mean dropping more than half of the entire data frame). In this case the entire columns will be dropped. This can be done by calling *drop_columns()* method from class DataFrameTransform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_dropped = [\"mths_since_last_delinq\", \"mths_since_last_record\", \"next_payment_date\", \"mths_since_last_major_derog\"]\n",
    "for column in columns_dropped:\n",
    "    df_transform.drop_columns(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check this by calling *show_colm_dtypes()* method from class DataFrameInfo which shows the names of all the columns in the data frame. In that list we will not see the columns that we just dropped. We can also check this by calling the *show_df_shape()* which shows the shape of the data frame. The number of columns should be decreased by 4 (from 44 columns to 40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_colm_dtypes()\n",
    "\n",
    "info.show_df_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have finished with dealing with Nan values, we can call *show_null_values()* method from class Plotter to visually check the work we did. The graph now should not contain any Nan values as well as the percentage of Nan values for each column should say 0%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visuals.show_null_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since none of the columns contain null values anymore, we can finish changing the data types (from float ('float64') to integer ('int64')) of columns which we could not change before due to them containing Nan values. Since we have dropped \"mths_since_last_delinq\", \"mths_since_last_record\" and \"mths_since_last_major_derog\", we are left with \"funded_amount\" and \"collections_12_mths_ex_med\" columns that still needs data type changing. This can be done by once again calling *float_to_int()* method from class DataTransform. Afterwards, we can check the result by calling *show_colm_dtypes()* method from class DataFrameInfo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = [\"funded_amount\", \"collections_12_mths_ex_med\"]\n",
    "for column in columns_list:\n",
    "    transform.float_to_int(column)\n",
    "    \n",
    "info.show_colm_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Make a copy of the data frame.\n",
    "<br>\n",
    "<br>\n",
    "Let us create a copy of a data frame to compare the results later in the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_df = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Perform transformations on skewed data. \n",
    "<br>\n",
    "<br>\n",
    "Data that is not symmetrically distributed around the mean (a.k.a. skewed data) can influence the interpretation of the data as many statistical techniques assume that the data is normally distributed. Once data is found to be skewed, data transformations may need to be performed to fix the skewness. \n",
    "\n",
    "The *show_skewness()* method from class Plotter will be used to calculate and illustrate the level of skewness in each column containing continuous data. \n",
    "<br>\n",
    "<br>\n",
    "These are the thresholds to determine the skewness of data:\n",
    " - between -0.5 and 0.5: the distribution of the data is **approximately symmetric**.\n",
    " - between -1 and -0.5 or between 0.5 and 1: the distribution of the data is **moderately skewed**. \n",
    " - less than -1 or greater than 1: the distribution of the data is **highly skewed**.</ul>\n",
    "\n",
    "The positive values indicate a **positive (right) skew**. This means that the mean and the median of the data will be less than the mode.\n",
    "<br>\n",
    "<br>\n",
    "The negative values indicate a **negative (left) skew**. This would indicate that the mean and the median of the data will be less than the mode. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew_columns = [\"loan_amount\", \"funded_amount\", \"funded_amount_inv\", \"int_rate\", \"instalment\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"open_accounts\", \"total_accounts\", \"out_prncp\", \"out_prncp_inv\", \"total_payment\", \"total_rec_prncp\"]\n",
    "for sk_colm in skew_columns:\n",
    "    visuals.show_skewness(sk_colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ease let us group the columns into three groups.\n",
    "\n",
    "*Columns containing symmetric data:*\n",
    " - int_rate\n",
    " - dti</ul>\n",
    "\n",
    "*Columns containing moderately skewed data:*\n",
    " - loan_amount\n",
    " - funded_amount\n",
    " - funded_amount_inv\n",
    " - instalment\n",
    " - total_accounts</ul>\n",
    "\n",
    "*Columns containing highly skewed data:*\n",
    " - annual_inc\n",
    " - delinq_2yrs\n",
    " - inq_last_6mths\n",
    " - open_accounts\n",
    " - out_prncp\n",
    " - out_prncp_inv\n",
    " - total_payment\n",
    " - total_rec_prncp</ul>\n",
    "\n",
    "It is interesting to note that all the columns containing skewed data are *positively skewed* (as all the skew values are positive).\n",
    "\n",
    "In order to transform columns containing moderately and highly skewed data, three methods of transformation were used: **Square Root**, **Log** and **Yeo-Johnson** transformation. All three methods were tried on each column to test which method transforms the data closest to the symmetric distribution. As a result three methods *transform_skew_square_root()*, *transform_skew_log()* and *transform_skew_yeojohnson()* were added in class DataFrameTransform to transform the moderately and highly skewed columns using the transformation method best fit to each column. \n",
    "\n",
    "*Square Root Transformation was used on:*\n",
    " - loan_amount\n",
    " - funded_amount\n",
    " - funded_amount_inv\n",
    " - instalment\n",
    " - total_accounts\n",
    " - total_payment\n",
    " - total_rec_prncp</ul>\n",
    "\n",
    "*Log Transformation was used on:*\n",
    " - annual_inc\n",
    " - open_accounts</ul>\n",
    "\n",
    "*Yeo-Johnson was used on:*\n",
    " - delinq_2yrs\n",
    " - inq_last_6mths\n",
    " - out_prncp\n",
    " - out_prncp_inv</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqr_columns = [\"loan_amount\", \"funded_amount\", \"funded_amount_inv\", \"instalment\", \"total_accounts\", \"total_payment\", \"total_rec_prncp\"]\n",
    "for clm in sqr_columns:\n",
    "    df_transform.transform_skew_square_root(clm)\n",
    "\n",
    "log_columns = [\"annual_inc\", \"open_accounts\"]\n",
    "for colm in log_columns:\n",
    "    df_transform.transform_skew_log(colm)\n",
    "\n",
    "yeo_columns = [\"delinq_2yrs\", \"inq_last_6mths\", \"out_prncp\", \"out_prncp_inv\"]\n",
    "for colmn in yeo_columns:\n",
    "    df_transform.transform_skew_yeojohnson(colmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the transformations, the *show_skewness()* method from class Plotter is called again to observe the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_columns = [\"loan_amount\", \"funded_amount\", \"funded_amount_inv\", \"instalment\", \"annual_inc\", \"delinq_2yrs\", \"inq_last_6mths\", \"open_accounts\", \"total_accounts\", \"out_prncp\", \"out_prncp_inv\", \"total_payment\", \"total_rec_prncp\"]\n",
    "for colm in corrected_columns:\n",
    "    visuals.show_skewness(colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the columns with skewed data were successfully transformed to have approximately symmetric data distribution except for \"delinq_2yrs\" column which still has highly skewed data (skewness was brought down from 5.32 to 1.87) as well as \"out_prncp\" and \"out_prncp_inv\" columns which still have moderately skewed data (for both the skewness was brought down from 2.35 to 0.53 (however the number is very close to the range of where data would be considered approximately symmetric)). This simply indicates that there might be some outliers present that needs to be dealt with.\n",
    "<br>\n",
    "<br>\n",
    "### 9. Remove outliers from the data. \n",
    "<br>\n",
    "<br>\n",
    "Outliers are data points that differ considerably from the majority of the other observations in the dataset. In other words, outliers are values in the data set that are very large or small compared to the majority of the values in the data set. Such values can negatively affect the statistical analysis (such as showing higher or lower mean and mode of the data) and the training process of a machine learning algorithm resulting in lower accuracy. Therefore it is important to deal with the outliers in the data. \n",
    "<br>\n",
    "\n",
    "In order to spot the outliers in categorical data, we can call *show_disc_prob_distr()* method from class Plotter which shows the value count and Discrete Probability Distribution of selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpd_columns = [\"term\", \"grade\", \"sub_grade\", \"employment_length\", \"home_ownership\", \"verification_status\", \"loan_status\", \"payment_plan\", \"purpose\", \"policy_code\", \"application_type\"]\n",
    "for clmn in dpd_columns:\n",
    "    show_dpd = visuals.show_disc_prob_distr(clmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"home_ownership\" column has a single value \"NONE\" which can be considered as an outlier. We can deal with it by replacing the \"NONE\" value to \"OTHER\" and therefore adding the value to other 63 \"OTHER\" values.\n",
    "<br>\n",
    "<br>\n",
    "The \"loan_status\" column has several outliers (353 values of \"Does not meet the credit policy. Status:Charged Off\" and 966 values of \"Does not meet the credit policy. Status:Fully Paid\") which like in \"home_ownership\" column case can be assigned to a similar group (to 5500 values of \"Charged Off\" and to 27011 values of \"Fully Paid, respectively).\n",
    "<br>\n",
    "<br>\n",
    "The \"payment_plan\" column has a clear outlier which is 1 value of \"y\" while the rest of the 54099 values are \"n\". As this outlier is extremely small in comparison to the rest of the data, it can dealt with by removing the a single row containing it. This will leave us with 54099 rows remaining in our data frame. \n",
    "<br>\n",
    "<br>\n",
    "Finally, while \"verification_status\" column does not have any visibly obvious outliers in the illustration, however we can group the values in the column in a more organised way. Logically, it does not make sense to have three groups of values called \"Source Verified\", \"Verified\" and \"Not Verified\" when \"Source Verified\" basically means the same thing as \"Verified\". So, all the \"Source Verified\" values will be replaced into \"Verified\" in order to group the values into two categories (\"Verified\" and \"Not Verified\").\n",
    "\n",
    "We can achieve all this by calling *categ_data_outliers_transform()* method from class DataFrameTransform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transform.categ_data_outliers_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check that the transformation was successful we can call the *show_disc_prob_distr()* method from class Plotter again while only specifying the names of the columns where transformations were made. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpd_columns = [\"home_ownership\", \"verification_status\", \"loan_status\", \"payment_plan\"]\n",
    "for clmn in dpd_columns:\n",
    "    show_dpd = visuals.show_disc_prob_distr(clmn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us move on to the outliers in the columns containing continuous data. By calling *show_cont_data_outliers()* method from class Plotter we can illustrate the outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cont_data_columns = ['loan_amount', 'funded_amount', 'funded_amount_inv', 'int_rate', 'instalment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_accounts', 'total_accounts', 'out_prncp', 'out_prncp_inv', 'total_payment', 'total_payment_inv', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', 'collection_recovery_fee', 'last_payment_amount', 'collections_12_mths_ex_med']\n",
    "for colm in Cont_data_columns:\n",
    "    show_cont_outliers = visuals.show_cont_data_outliers(colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the illustrations above, we can conclude that columns \"loan_amount\", \"funded_amount\", \"out_prncp\" and \"out_prncp_inv\" do not have obvious outliers therefore these columns do not require outliers' removal. The \"collections_12_mths_ex_med\" visually seems to have four groups of outliers, however as it's majority of values are 0, we should not rush to remove the outliers. The *value_counts()* method should provide us the insight of the groups of values that the column contains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw[\"collections_12_mths_ex_med\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the majority of values in \"collections_12_mths_ex_med\" are 0, the other 4 unique values (1.0, 2.0, 3.0.,4.0) are considered outliers. If we would decide to remove the outliers or replace them with a median or mean (which would be 0 again), the data would be lost as all the values would just be 0. As a result, we will not deal with the outliers in this column.  \n",
    "<br>\n",
    "<br>\n",
    "The columns named \"funded_amount_inv\", delinq_2yrs\", \"total_rec_prncp\", \"int_rate\", \"instalment\", \"dti\" and \"total payment\" have relatively small outliers which are located either above (indicating extremely high numbers are the outliers) or below (indicating extremely low numbers are the outliers) the box plot illustration. Due to the relatively small number of outliers, these outliers can be dealt with by simply eliminating them. We can do this by calling *iqr_outliers_removal()* method from class DataFrameTransform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iqr_columns = [\"funded_amount_inv\", \"delinq_2yrs\", \"total_rec_prncp\", \"int_rate\", \"instalment\", \"dti\", \"total_payment\"]\n",
    "for iqr_colm in iqr_columns:\n",
    "    df_transform.iqr_outliers_removal(iqr_colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are eliminating the rows containing the outliers, we can check whether the outlier removal procedure worked by checking the length of the data frame before the outlier removal and after. As the length of the data frame have decreased from 54099 to 52088, this proves that outliers were successfully removed. \n",
    "<br>\n",
    "<br>\n",
    "Columns such as \"annual_inc\", \"open_accounts\" and \"total_accounts\" have quite a significant amount of outliers from both sides of the box plot. The outlier removal procedure (like the one used before) cannot be applied here as in this case quite a significant amount of rows would be lost from the data frame. Instead we can use a method where we set the flooring (the minimum value) and capping (the maximum value) for the dataset. This means that we would replace the data values that are below the 10th percentile with the 10th percentile values as well as replace the data values that are above the 90th percentile with 90th percentile values. This can be done by calling *flooring_capping_outliers_removal()* method from class DataFrameTransform. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floor_cap_columns = [\"annual_inc\", \"open_accounts\", \"total_accounts\"]\n",
    "print(f'The length of data frame BEFORE capping and flooring of outliers: {len(df_raw)}\\n')\n",
    "for fl_cp_column in floor_cap_columns:\n",
    "    df_transform.flooring_capping_outliers_transform(fl_cp_column)\n",
    "print(f'The length of data frame AFTER the capping and flooring of outliers: {len(df_raw)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The identical length of data frame before and after the outliers transformation confirms that we only intended here to replace the outliers rather than remove them, therefore the length of the data frame should not change. Once the flooring and capping values are applied, these values should also be the new minimum and maximum values of each column. If that is the case, this means the transformation was successful. \n",
    "<br>\n",
    "<br>\n",
    "The remaining columns \"total_rec_int\", \"total_rec_late_fee\", \"recoveries\", \"collection_recovery_fee\", \"last_payment_amount\" and \"total_payment_inv\" contains very large amount of outliers above the box plot. Due to the large number of outliers there, it would not make sense to either remove or replace them because in both cases a large percentage of data values would be removed or changed, which can influence the interpretation of the data later on. As a result, we will not deal with the outliers for these columns. \n",
    "<br>\n",
    "<br>\n",
    "By calling the *show_cont_data_outliers()* method again and selecting only the column names in which the transformation took place, we should be able to see a drastically reduced amount of outliers present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cont_data_columns = [\"funded_amount_inv\", \"delinq_2yrs\", \"total_rec_prncp\", \"int_rate\", \"instalment\", \"dti\", \"total_payment\", \"annual_inc\", \"open_accounts\", \"total_accounts\"]\n",
    "for colm in Cont_data_columns:\n",
    "    show_cont_outliers = visuals.show_cont_data_outliers(colm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Dropping overly correlated columns. \n",
    "<br>\n",
    "<br>\n",
    "**Collinearity** occurs when two independent variables are heavily related to each other to extend where one of the independent variables can be predicted from the others with a very high degree of accuracy. **Multicollinearity** occurs when more than two independent variables are heavily related to each other. \n",
    "<br>\n",
    "As collinearity occurs over the linear relationship between variables, we can call *show_correlation_cont_data()* method from Plotter which uses a correlation heatmap to illustrate the correlation between independent variables in our the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = [\"loan_amount\", \"funded_amount\", \"funded_amount_inv\", \"int_rate\", \"instalment\", \"annual_inc\", \"dti\", \"delinq_2yrs\", \"inq_last_6mths\", \"open_accounts\", \"total_accounts\", \"out_prncp\", \"out_prncp_inv\", \"total_payment\", \"total_payment_inv\", \"total_rec_prncp\", \"total_rec_int\", \"total_rec_late_fee\",\"recoveries\", \"collection_recovery_fee\", \"last_payment_amount\", \"collections_12_mths_ex_med\"]\n",
    "visuals.show_correlation_cont_data(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no strict rule for how much collinearity is too much, however a rule of thumb of anything above 0.9 is often used. After analysing the correlation heatmap, we can summarise the columns that have high correlation with other columns:\n",
    "<br>\n",
    " - **\"loan_amount\"** is correlated with:&ensp; *\"funded_amount\"(0.966)*,&emsp;&emsp;&ensp; *\"funded_amount_inv\"(0.963)*,&emsp; *\"instalment\"(0.957)*.\n",
    " - **\"funded_amount\"** with:&emsp;&emsp;&emsp;&emsp;&emsp;*\"funded_amount_inv\"(0.942)*,&emsp; *\"instalment\"(0.937)*.\n",
    " - **\"funded_amount_inv\"** with:&emsp;&emsp;&emsp;*\"instalment\"*.\n",
    " - **\"out_prncp\"** with: &emsp; &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;*\"out_prncp_inv\"(0.999)*\n",
    " - **\"total_payment\"** with:  &emsp; &emsp;&emsp;&emsp;&emsp;*\"total_payment_inv\"(0.960)*,&emsp;&ensp; *\"total_rec_prncp\"(0.957)*.\n",
    " - **\"total_payment_inv\"** with:  &emsp;&emsp;&emsp;&ensp;*\"total_rec_prncp\"(0.915)*.</ul>\n",
    "<br>\n",
    "Note: In above when I mention, for example, a column_one is correlated with column_two, it also means column_two is correlated with column_one. By assuming this, I can avoid repeating myself later by saying, for example, column_two is related with column_one. \n",
    "<br>\n",
    "<br>\n",
    "The first thing we can notice is that there is multicollinearity as *\"instalment\"* has correlation with three other columns (\"loan_amount\", \"funded_amount\" and \"funded_amount_inv\"), *\"funded_amount_inv\"* has correlation with two other columns (\"loan_amount\" and \"funded_amount\") and \"total_rec_prncp\" has correlation with two other columns as well (\"total_payment\" and \"total_payment_inv\"). As a result, it make sense to drop these columns first. \n",
    "<br>\n",
    "<br>\n",
    "Next, we can notice that  *\"out_prncp\"* and *\"out_prncp_inv\"* are nearly perfectly collinear (the correlation number is very close to 1). This means that with the correlation as strong as this, it would be more difficult to change the data in one of the columns without changing the data in the other one and this can complicate the interpretation of the data. Therefore one of the columns should be removed. In order to decided which column to remove, we should analyse what the values in each column really represent:\n",
    "<br>\n",
    "<br>\n",
    "<ins>out_prncp:</ins> Remaining outstanding principal(remaining amount of the original loan + any capitalized interest) for total amount funded.\n",
    "<br>\n",
    "<br>\n",
    "<ins>out_prncp_inv:</ins> Remaining outstanding principal for portion of total amount funded by investors. \n",
    "<br>\n",
    "<br>So \"out_prncp\" displays the remaining amount of the original loan for total amount funded (funded by anyone, therefore, a more general case) while \"out_prncp_inv\" displays the remaining amount of the original loan for portion of total amount, that is funded by investors (a more specific case that involves 'investors' as a specific group rather than 'anyone'). In this case it could make sense to drop the \"out_prncp_inv\" column in order to leave us with a more general case of \"out_prncp\" column data which could be more useful for later analysis. \n",
    "<br>\n",
    "<br>\n",
    "Finally, we have \"loan_amount\" column correlating with \"funded_amount\" and then \"total_payment\" correlating with \"total_payment_inv\". However, here we need to remember that we should not be dropping too many correlated columns as then, in some situations, we may be subjecting our data to omitted variable bias which means that by deleting the correlated column we can lose important features from that column that are needed for effective analysis of certain topics. \n",
    "<br>\n",
    "<br>\n",
    "A *drop_columns()* method from class DataFrameTransform can be used to drop the columns and then we can call *show_df_shape()* method from class DataFrameInfo to check the shape of the data frame before and after the removal of columns to confirm that the removal is completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.show_df_shape()\n",
    "\n",
    "columns_dropped = [\"instalment\", \"funded_amount_inv\", \"total_rec_prncp\", \"out_prncp_inv\"]\n",
    "for column in columns_dropped:\n",
    "    df_transform.drop_columns(column)\n",
    "    \n",
    "info.show_df_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that the column removal is complete by observing the number of columns decreasing from 40 to 36 columns. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
